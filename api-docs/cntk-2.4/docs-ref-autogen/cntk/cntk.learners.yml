### YamlMime:PythonPackage
uid: cntk.learners
name: learners
fullName: cntk.learners
summary: 'A learner tunes a set of parameters during the training process. One can
  use

  different learners for different sets of parameters. Currently, CNTK supports

  the following learning algorithms:



  - <xref:adadelta>

  - <xref:adagrad>

  - <xref:fsadagrad>

  - <xref:adam>

  - <xref:momentum_sgd>

  - <xref:nesterov>

  - <xref:rmsprop>

  - <xref:sgd>

  - <xref:universal>'
type: import
functions:
- uid: cntk.learners.adadelta
  name: adadelta
  summary: "Creates an AdaDelta learner instance to learn the parameters. See [1]\
    \ for\nmore information.\n\nSee also\n   [1]  Matthew D. Zeiler, [ADADELTA: An\
    \ Adaptive Learning Rate Method](https://arxiv.org/pdf/1212.5701.pdf)."
  signature: adadelta(parameters, lr, rho, epsilon, l1_regularization_weight=0, l2_regularization_weight=0,
    gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
    gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.learning_parameter_schedule>
  - name: rho
    description: exponential smooth factor for each minibatch.
    types:
    - <xref:float>
  - name: epsilon
    description: epsilon for sqrt.
    types:
    - <xref:float>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
- uid: cntk.learners.adagrad
  name: adagrad
  summary: 'Creates an AdaGrad learner instance to learn the parameters. See [1] for

    more information.'
  signature: adagrad(parameters, lr, need_ave_multiplier=True, l1_regularization_weight=0,
    l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
    gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.learning_parameter_schedule>
  - name: need_ave_multiplier
    types:
    - <xref:bool>, <xref:default>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
  seeAlsoContent: "  [1]  J. Duchi, E. Hazan, and Y. Singer. [Adaptive Subgradient\
    \ Methods\n  for Online Learning and Stochastic Optimization](http://www.magicbroom.info/Papers/DuchiHaSi10.pdf).\
    \ The Journal of\n  Machine Learning Research, 2011.\n"
- uid: cntk.learners.adam
  name: adam
  summary: 'Creates an Adam learner instance to learn the parameters. See [1] for
    more

    information.'
  signature: 'adam(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
    variance_momentum=momentum_schedule_per_sample(0.9999986111120757), l1_regularization_weight=0,
    l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
    gradient_clipping_with_truncation=True, epsilon=1e-8, adamax=False)'
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.learning_parameter_schedule>
  - name: momentum
    description: 'momentum schedule. Note that this is the beta1 parameter in the
      Adam paper [1].

      For additional information, please refer to the [this CNTK Wiki article](/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.momentum_schedule>
  - name: unit_gain
    description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

      to the value returned by <xref:cntk.learners.default_unit_gain_value>.'
  - name: variance_momentum
    description: 'variance momentum schedule.

      Note that this is the beta2 parameter in the Adam paper [1]. Defaults to `momentum_schedule_per_sample(0.9999986111120757)`.'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.momentum_schedule>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: epsilon
    description: 'numerical stability constant,

      defaults to 1e-8'
    types:
    - <xref:float>, <xref:optional>
  - name: adamax
    description: 'when `True`, use infinity-norm variance momentum update instead
      of L2. Defaults

      to False'
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate, momentum
      and variance_momentum. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
  seeAlsoContent: "  [1] D. Kingma, J. Ba. [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980).\
    \ International Conference for\n  Learning Representations, 2015.\n"
- uid: cntk.learners.default_unit_gain_value
  name: default_unit_gain_value
  summary: Returns true if by default momentum is applied in the unit-gain fashion.
  signature: default_unit_gain_value()
- uid: cntk.learners.fsadagrad
  name: fsadagrad
  summary: Creates an FSAdaGrad learner instance to learn the parameters.
  signature: fsadagrad(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
    variance_momentum=momentum_schedule_per_sample(0.9999986111120757), l1_regularization_weight=0,
    l2_regularization_weight=0, gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
    gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.learning_parameter_schedule>
  - name: momentum
    description: 'momentum schedule.

      For additional information, please refer to the [this CNTK Wiki article](/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.momentum_schedule>
  - name: unit_gain
    description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

      to the value returned by <xref:cntk.learners.default_unit_gain_value>.'
  - name: variance_momentum
    description: 'variance momentum schedule. Defaults

      to `momentum_schedule_per_sample(0.9999986111120757)`.'
    types:
    - <xref:float>, <xref:list>, <xref:output of cntk.learners.momentum_schedule>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate, momentum
      and variance_momentum. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
- uid: cntk.learners.learning_parameter_schedule
  name: learning_parameter_schedule
  summary: Create a learning parameter schedule.
  signature: learning_parameter_schedule(schedule, minibatch_size=None, epoch_size=None)
  parameters:
  - name: schedule
    description: 'if float, is the parameter schedule to be used

      for all samples. In case of list [p_1, p_2, .., p_n], the i-th parameter p_i
      in the list is used as the

      value from the (`epoch_size` * (i-1) + 1)-th sample to the (`epoch_size` * i)-th
      sample. If list contains

      pair, i.e. [(num_epoch_1, p_1), (num_epoch_n, p_2), .., (num_epoch_n, p_n)],
      the i-th parameter is used as a

      value from the (`epoch_size` * (num_epoch_0 + ... + num_epoch_2 + ... + num_epoch_(i-1)
      + 1)-th sample to the

      (`epoch_size` * num_epoch_i)-th sample (taking num_epoch_0 = 0 as a special
      initialization).'
    types:
    - <xref:float>
    - <xref:list>
  - name: minibatch_size
    description: 'an integer to specify the minibatch size that schedule are designed
      for.

      CNTK will scale the schedule internally so as to simulate the behavior of the
      schedule as much as possible

      to match the designed effect. If it is not specified, CNTK will set to the special
      value <xref:cntk.learners.IGNORE>.'
    types:
    - <xref:int>
  - name: epoch_size
    description: 'number of samples as a scheduling unit.

      Parameters in the schedule change their values every `epoch_size`

      samples. If no `epoch_size` is provided, this parameter is substituted

      by the size of the full data sweep, in which case the scheduling unit is

      the entire data sweep (as indicated by the MinibatchSource) and parameters

      change their values on the sweep-by-sweep basis specified by the

      `schedule`.'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: learning parameter schedule
- uid: cntk.learners.learning_parameter_schedule_per_sample
  name: learning_parameter_schedule_per_sample
  summary: 'Create a learning parameter schedule as if the parameter is applied to
    minibatches of size 1. CNTK

    will scale the parameters accordingly with respect to the actual minibatch size.'
  signature: learning_parameter_schedule_per_sample(schedule, epoch_size=None)
  parameters:
  - name: schedule
    description: 'if float, is the parameter schedule to be used

      for all samples. In case of list [p_1, p_2, .., p_n], the i-th parameter p_i
      in the list is used as the

      value from the (`epoch_size` * (i-1) + 1)-th sample to the (`epoch_size` * i)-th
      sample. If list contains

      pair, i.e. [(num_epoch_1, p_1), (num_epoch_n, p_2), .., (num_epoch_n, p_n)],
      the i-th parameter is used as a

      value from the (`epoch_size` * (num_epoch_0 + ... + num_epoch_2 + ... + num_epoch_(i-1)
      + 1)-th sample to the

      (`epoch_size` * num_epoch_i)-th sample (taking num_epoch_0 = 0 as a special
      initialization).'
    types:
    - <xref:float>
    - <xref:list>
  - name: epoch_size
    description: 'number of samples as a scheduling unit.

      Parameters in the schedule change their values every `epoch_size`

      samples. If no `epoch_size` is provided, this parameter is substituted

      by the size of the full data sweep, in which case the scheduling unit is

      the entire data sweep (as indicated by the MinibatchSource) and parameters

      change their values on the sweep-by-sweep basis specified by the

      `schedule`.'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: learning parameter schedule as if it is applied to minibatches of
      size 1.
- uid: cntk.learners.learning_rate_schedule
  name: learning_rate_schedule
  summary: 'Deprecated:: 2.2


    Create a learning rate schedule (using the same semantics as

    <xref:cntk.learners.training_parameter_schedule>).'
  signature: learning_rate_schedule(lr, unit, epoch_size=None)
  parameters:
  - name: lr
    description: 'see parameter `schedule` in

      <xref:cntk.learners.training_parameter_schedule>.'
    types:
    - <xref:float>
    - <xref:list>
  - name: unit
    description: "see parameter\n`unit` in <xref:cntk.learners.training_parameter_schedule>.\n\
      \n   deprecated:: 2.2\n      Use minibatch_size parameter to specify the reference\
      \ minbiatch size instead."
    types:
    - <xref:cntk.learners.UnitType>
  - name: epoch_size
    description: 'see parameter `epoch_size` in

      <xref:cntk.learners.training_parameter_schedule>.'
    types:
    - <xref:int>
  return:
    description: learning rate schedule
  seeAlsoContent: '  <xref:cntk.learners.training_parameter_schedule>

    '
- uid: cntk.learners.momentum_as_time_constant_schedule
  name: momentum_as_time_constant_schedule
  summary: "Create a momentum schedule in a minibatch-size agnostic way\n(using the\
    \ same semantics as <xref:cntk.learners.training_parameter_schedule>\nwith *unit=UnitType.sample*).\n\
    \nDeprecated:: 2.2\n   This is for legacy API.\n   In this legacy API,:\n\n  \
    \ <!-- literal_block {\"ids\": [], \"classes\": [], \"names\": [], \"dupnames\"\
    : [], \"backrefs\": [], \"xml:space\": \"preserve\"} -->\n\n   ````\n\n      #assume\
    \ the desired minibatch size invariant constant momentum rate is: momentum_rate\n\
    \      momentum_time_constant = -minibatch_size/np.log(momentum_rate)\n      momentum\
    \ = momentum_as_time_constant_schedule(momentum_time_constant)\n      ````\n\n\
    \   The equivalent code in the latest API,\n\n   <!-- literal_block {\"ids\":\
    \ [], \"classes\": [], \"names\": [], \"dupnames\": [], \"backrefs\": [], \"xml:space\"\
    : \"preserve\"} -->\n\n   ````\n\n      momentum = momentum_schedule(momentum_rate,\
    \ minibatch_size = minibatch_size)\n      ````\n\nCNTK specifies momentum in a\
    \ minibatch-size agnostic way as the time\nconstant (in samples) of a unit-gain\
    \ 1st-order IIR filter. The value\nspecifies the number of samples after which\
    \ a gradient has an effect of\n1/e=37%.\n\nIf you want to specify the momentum\
    \ per N samples (or per minibatch),\nuse <xref:cntk.learners.momentum_schedule>."
  signature: momentum_as_time_constant_schedule(momentum, epoch_size=None)
  return:
    description: momentum as time constant schedule
  examples:
  - '

    ```


    >>> # Use a fixed momentum of 1100 for all samples

    >>> m = momentum_as_time_constant_schedule(1100)

    ```



    ```


    >>> # Use the time constant 1100 for the first 1000 samples,

    >>> # then 1500 for the remaining ones

    >>> m = momentum_as_time_constant_schedule([1100, 1500], 1000)

    ```

    '
- uid: cntk.learners.momentum_schedule
  name: momentum_schedule
  summary: 'Create a momentum schedule (using the same semantics as

    <xref:cntk.learners.learning_parameter_schedule>) which applies the momentum

    decay every N samples where N is specified by the argument *minibatch_size*.'
  signature: momentum_schedule(momentum, epoch_size=None, minibatch_size=None)
  return:
    description: momentum schedule
  examples:
  - '

    ```


    >>> # Use a fixed momentum of 0.99 for all samples

    >>> m = momentum_schedule(0.99)

    ```



    ```


    >>> # Use the momentum value 0.99 for the first 1000 samples,

    >>> # then 0.9 for the remaining ones

    >>> m = momentum_schedule([0.99,0.9], 1000)

    >>> m[0], m[999], m[1000], m[1001]

    (0.99, 0.99, 0.9, 0.9)

    ```



    ```


    >>> # Use the momentum value 0.99 for the first 999 samples,

    >>> # then 0.88 for the next 888 samples, and 0.77 for the

    >>> # the remaining ones

    >>> m = momentum_schedule([(999,0.99),(888,0.88),(0, 0.77)])

    >>> m[0], m[998], m[999], m[999+888-1], m[999+888]

    (0.99, 0.99, 0.88, 0.88, 0.77)

    ```

    '
- uid: cntk.learners.momentum_schedule_per_sample
  name: momentum_schedule_per_sample
  summary: 'Create a per-sample momentum schedule (using the same semantics as

    <xref:cntk.learners.momentum_schedule> but specializing in per sample momentum
    schedule).'
  signature: momentum_schedule_per_sample(momentum, epoch_size=None)
  parameters:
  - name: momentum
    description: 'see parameter `schedule` in

      <xref:cntk.learners.training_parameter_schedule>.'
    types:
    - <xref:float>
    - <xref:list>
  - name: epoch_size
    description: 'see parameter `epoch_size` in

      <xref:cntk.learners.momentum_schedule>.'
    types:
    - <xref:int>
  return:
    description: momentum schedule
- uid: cntk.learners.momentum_sgd
  name: momentum_sgd
  summary: Creates a Momentum SGD learner instance to learn the parameters.
  signature: momentum_sgd(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
    l1_regularization_weight=0.0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
    gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.learning_parameter_schedule>>
  - name: momentum
    description: 'momentum schedule.

      For additional information, please refer to the [this CNTK Wiki article](/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.momentum_schedule>>
  - name: unit_gain
    description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

      to the value returned by <xref:cntk.learners.default_unit_gain_value>.'
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate and momentum.
      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
- uid: cntk.learners.nesterov
  name: nesterov
  summary: 'Creates a Nesterov SGD learner instance to learn the parameters. This
    was

    originally proposed by Nesterov [1] in 1983 and then shown to work well in

    a deep learning context by Sutskever, et al. [2].'
  signature: nesterov(parameters, lr, momentum, unit_gain=default_unit_gain_value(),
    l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
    gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.learning_parameter_schedule>>
  - name: momentum
    description: 'momentum schedule.

      For additional information, please refer to the [this CNTK Wiki article](/cognitive-toolkit/BrainScript-SGD-Block#converting-learning-rate-and-momentum-parameters-from-other-toolkits).'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.momentum_schedule>>
  - name: unit_gain
    description: 'when `True`, momentum is interpreted as a unit-gain filter. Defaults

      to the value returned by <xref:cntk.learners.default_unit_gain_value>.'
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate and momentum.
      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
  seeAlsoContent: "  [1] Y. Nesterov. A Method of Solving a Convex Programming Problem\
    \ with Convergence Rate O(1/ sqrt(k)). Soviet Mathematics Doklady, 1983.\n\n \
    \ [2] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. [On the\n  Importance\
    \ of Initialization and Momentum in Deep Learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf).\
    \  Proceedings\n  of the 30th International Conference on Machine Learning, 2013.\n"
- uid: cntk.learners.rmsprop
  name: rmsprop
  summary: Creates an RMSProp learner instance to learn the parameters.
  signature: rmsprop(parameters, lr, gamma, inc, dec, max, min, need_ave_multiplier=True,
    l1_regularization_weight=0, l2_regularization_weight=0, gaussian_noise_injection_std_dev=0,
    gradient_clipping_threshold_per_sample=np.inf, gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s `parameters`.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.learning_parameter_schedule>>
  - name: gamma
    description: 'Trade-off factor for current and previous gradients. Common value
      is 0.95. Should be in range (0.0, 1.0)'
    types:
    - <xref:float>
  - name: inc
    description: Increasing factor when trying to adjust current learning_rate. Should
      be greater than 1
    types:
    - <xref:float>
  - name: dec
    description: Decreasing factor when trying to adjust current learning_rate. Should
      be in range (0.0, 1.0)
    types:
    - <xref:float>
  - name: max
    description: Maximum scale allowed for the initial learning_rate. Should be greater
      than zero and min
    types:
    - <xref:float>
  - name: min
    description: Minimum scale allowed for the initial learning_rate. Should be greater
      than zero
    types:
    - <xref:float>
  - name: need_ave_multiplier
    types:
    - <xref:bool>, <xref:default True>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
- uid: cntk.learners.set_default_unit_gain_value
  name: set_default_unit_gain_value
  summary: Sets globally default unit-gain flag value.
  signature: set_default_unit_gain_value(value)
  parameters:
  - name: value
- uid: cntk.learners.sgd
  name: sgd
  summary: 'Creates an SGD learner instance to learn the parameters. See [1] for more

    information on how to set the parameters.'
  signature: sgd(parameters, lr, l1_regularization_weight=0, l2_regularization_weight=0,
    gaussian_noise_injection_std_dev=0, gradient_clipping_threshold_per_sample=np.inf,
    gradient_clipping_with_truncation=True)
  parameters:
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the ''.parameters()'' method of the root

      operator.'
    types:
    - <xref:list of parameters>
  - name: lr
    description: 'a learning rate in float, or a learning rate schedule.

      See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:float>, <xref:list>, <xref:output of <xref:cntk.learners.learning_parameter_schedule>>
  - name: l1_regularization_weight
    description: 'the L1 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: l2_regularization_weight
    description: 'the L2 regularization weight per sample,

      defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gaussian_noise_injection_std_dev
    description: 'the standard deviation

      of the Gaussian noise added to parameters post update, defaults to 0.0'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_threshold_per_sample
    description: 'clipping threshold

      per sample, defaults to infinity'
    types:
    - <xref:float>, <xref:optional>
  - name: gradient_clipping_with_truncation
    description: 'use gradient clipping

      with truncation'
    types:
    - <xref:bool>, <xref:default True>
  - name: use_mean_gradient
    description: "use averaged gradient as input to learner.\n\ndeprecated:: 2.2\n\
      \   Use minibatch_size parameter to specify the reference minibatch size."
    types:
    - <xref:bool>, <xref:optional>
  - name: minibatch_size
    description: 'The minibatch size that the learner''s parameters are designed or
      pre-tuned for. This

      size is usually set to the same as the minibatch data source''s size. CNTK will
      perform automatic scaling of the parameters

      to enable efficient model parameter update implementation while approximate
      the behavior of pre-designed and pre-tuned parameters.

      In case that minibatch_size is not specified, CNTK will inherit the minibatch
      size from the learning rate schedule;

      if the learning rate schedule does not specify the minibatch_size, CNTK will
      set it to <xref:cntk.learners.IGNORE>. Setting minibatch_size to <xref:cntk.learners.IGNORE>

      will have the learner apply as it is preventing CNTK performing any hyper-parameter
      scaling. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:int>, <xref:default None>
  - name: epoch_size
    description: 'number of samples as a scheduling unit for learning rate. See also:  <xref:cntk.learners.learning_parameter_schedule>'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
  seeAlsoContent: "  [1] L. Bottou. [Stochastic Gradient Descent Tricks](https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks).\
    \ Neural\n  Networks: Tricks of the Trade: Springer, 2012.\n"
- uid: cntk.learners.training_parameter_schedule
  name: training_parameter_schedule
  summary: 'Deprecated:: 2.2


    Create a training parameter schedule containing either per-sample (default)

    or per-minibatch values.'
  signature: 'training_parameter_schedule(schedule, unit=<UnitType.minibatch: ''minibatch''>,
    epoch_size=None)'
  parameters:
  - name: schedule
    description: 'if float, is the parameter schedule to be used

      for all samples. In case of list, the elements are used as the

      values for `epoch_size` samples. If list contains pair, the second element is

      used as a value for (`epoch_size` x first element) samples'
    types:
    - <xref:float>
    - <xref:list>
  - name: unit
    description: "one of two\n* `sample`: the returned schedule contains per-sample\
      \ values\n* `minibatch`: the returned schedule contains per-minibatch values.\n\
      \n   deprecated:: 2.2\n      Use minibatch_size parameter to specify the reference\
      \ minbiatch size."
    types:
    - <xref:cntk.learners.UnitType>
  - name: epoch_size
    description: 'number of samples as a scheduling unit.

      Parameters in the schedule change their values every `epoch_size`

      samples. If no `epoch_size` is provided, this parameter is substituted

      by the size of the full data sweep, in which case the scheduling unit is

      the entire data sweep (as indicated by the MinibatchSource) and parameters

      change their values on the sweep-by-sweep basis specified by the

      `schedule`.'
    types:
    - <xref:optional>, <xref:int>
  return:
    description: training parameter schedule
  examples:
  - '

    ```


    >>> # Use a fixed value 0.01 for all samples

    >>> s = training_parameter_schedule(0.01)

    >>> s[0], s[1]

    (0.01, 0.01)

    ```



    ```


    >>> # Use 0.01 for the first 1000 samples, then 0.001 for the remaining ones

    >>> s = training_parameter_schedule([0.01, 0.001], epoch_size=1000)

    >>> s[0], s[1], s[1000], s[1001]

    (0.01, 0.01, 0.001, 0.001)

    ```



    ```


    >>> # Use 0.1 for the first 12 epochs, then 0.01 for the next 15,

    >>> # followed by 0.001 for the remaining ones, with a 100 samples in an epoch

    >>> s = training_parameter_schedule([(12, 0.1), (15, 0.01), (1, 0.001)], epoch_size=100)

    >>> s[0], s[1199], s[1200], s[2699], s[2700], s[5000]

    (0.1, 0.1, 0.01, 0.01, 0.001, 0.001)

    ```

    '
  seeAlsoContent: '  <xref:cntk.learners.learning_parameter_schedule>

    '
- uid: cntk.learners.universal
  name: universal
  summary: Creates a learner which uses a CNTK function to update the parameters.
  signature: universal(update_func, parameters)
  parameters:
  - name: update_func
    description: 'function that takes parameters and gradients as arguments and

      returns a <xref:cntk.ops.functions.Function> that performs the

      desired updates. The returned function updates the parameters by

      means of containing <xref:cntk.ops.assign> operations.

      If `update_func` does not contain <xref:cntk.ops.assign> operations

      the parameters will not be updated.'
  - name: parameters
    description: 'list of network parameters to tune.

      These can be obtained by the root operator''s *parameters*.'
    types:
    - <xref:list>
  return:
    description: 'learner instance that can be passed to

      the <xref:cntk.train.trainer.Trainer>'
    types:
    - <xref:cntk.learners.Learner>
  examples:
  - '

    ```


    >>> def my_adagrad(parameters, gradients):

    ...     accumulators = [C.constant(0, shape=p.shape, dtype=p.dtype, name=''accum'')
    for p in parameters]

    ...     update_funcs = []

    ...     for p, g, a in zip(parameters, gradients, accumulators):

    ...         accum_new = C.assign(a, g * g)

    ...         update_funcs.append(C.assign(p, p - 0.01 * g / C.sqrt(accum_new +
    1e-6)))

    ...     return C.combine(update_funcs)

    ...

    >>> x = C.input_variable((10,))

    >>> y = C.input_variable((2,))

    >>> z = C.layers.Sequential([C.layers.Dense(100, activation=C.relu), C.layers.Dense(2)])(x)

    >>> loss = C.cross_entropy_with_softmax(z, y)

    >>> learner = C.universal(my_adagrad, z.parameters)

    >>> trainer = C.Trainer(z, loss, learner)

    >>> # now trainer can be used as any other Trainer

    ```

    '
classes:
- cntk.learners.Learner
- cntk.learners.UserLearner
packages:
- cntk.learners.tests
enums:
- cntk.learners.UnitType
